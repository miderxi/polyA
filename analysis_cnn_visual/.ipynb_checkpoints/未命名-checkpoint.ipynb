{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grad-CAM，Gradient Weighted Class Activation Map\n",
    "Grad-CAM,可以不用重新训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算Grad-CAM的公式\n",
    "${\\alpha}_k^{c} = \\frac{1}{Z}\\sum\\limits_{i}\\sum\\limits_{j} \\frac{\\partial {y}^{c}}{\\partial {A}_{ij}^{k} } \n",
    "$<br />\n",
    "${L}^{c}grad-CAM = Relu(\\sum\\limits_{k}\\alpha_k^c A^k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (8844, 200, 4) (2948, 200, 4) (2948, 200, 4)\n",
      "Y: (8844,) (2948,) (2948,)\n"
     ]
    }
   ],
   "source": [
    "#读取数据\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import Input,layers\n",
    "from keras import Model\n",
    "from keras import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "import os\n",
    "\n",
    "polys=\"AATAGA AATATA CATAAA GATAAA AGTAAA ACTAAA TATAAA AATACA AAAAAG AAGAAA ATTAAA AATAAA\".split(\" \")\n",
    "np.random.seed(22)\n",
    "\n",
    "def get_data(poly_name,file_dir='../data/'):\n",
    "    '''\n",
    "    poly_name:是polya的名字\n",
    "    返回该polya的正负数据与标签\n",
    "    '''\n",
    "    file_path_pos = file_dir+poly_name+'.txt'\n",
    "    file_path_neg = file_dir+'neg'+poly_name+'.txt'\n",
    "    base2num={\n",
    "    'A':-2,'T':-1,'C':1,'G':2,\n",
    "    'a':-2,'t':-1,'c':1,'g':2\n",
    "    }\n",
    "    \n",
    "    base2num={\n",
    "    'A':np.array([1,0,0,0],dtype='float16'),\n",
    "    'T':np.array([0,1,0,0],dtype='float16'),\n",
    "    'C':np.array([0,0,1,0],dtype='float16'),\n",
    "    'G':np.array([0,0,0,1],dtype='float16'),\n",
    "    'a':np.array([1,0,0,0],dtype='float16'),\n",
    "    't':np.array([0,1,0,0],dtype='float16'),\n",
    "    'c':np.array([0,0,1,0],dtype='float16'),\n",
    "    'g':np.array([0,0,0,1],dtype='float16')\n",
    "    }\n",
    "    \n",
    "    \n",
    "    pdata = np.loadtxt(file_path_pos,dtype='str')\n",
    "    pdata = [seq[:100]+seq[106:] for seq in pdata]\n",
    "    pdata = [[base2num[base] for base in seq] for seq in pdata]\n",
    "                   \n",
    "    ndata = np.loadtxt(file_path_neg,dtype='str')\n",
    "    ndata = [seq[:100]+seq[106:] for seq in ndata]\n",
    "    ndata = [[base2num[base] for base in seq] for seq in ndata]\n",
    "    \n",
    "    X = np.array(pdata+ndata)\n",
    "    y = np.append(np.ones(len(pdata)),np.zeros(len(ndata)))\n",
    "    \n",
    "    return X,y\n",
    "\n",
    "def assess(y_true,y_prob,roc=True,pr=False,poly_name=None,line_name=None):\n",
    "    '''\n",
    "    pass\n",
    "    '''\n",
    "    re={}\n",
    "    y_true = np.array(y_true,dtype=int)\n",
    "    y_pred = np.array(y_prob+0.5,dtype=int)\n",
    "    \n",
    "    re['accuracy'] = round(metrics.accuracy_score  (y_true,y_pred),3)\n",
    "    re['precision'] = round(metrics.precision_score(y_true,y_pred),3)\n",
    "    re['recall'] = round(metrics.recall_score      (y_true,y_pred),3)\n",
    "    re['f1'] =  round(metrics.f1_score             (y_true,y_pred),3)\n",
    "    re['auc'] = round(metrics.roc_auc_score        (y_true, y_prob),3)\n",
    "    \n",
    "    if (poly_name != None):\n",
    "        my_label = poly_name+': '+str(re['auc'])\n",
    "    elif(line_name != None):\n",
    "        my_label = line_name\n",
    "    else:\n",
    "        my_label = 'roc'\n",
    "    \n",
    "    fpr,tpr,thresholds = metrics.roc_curve(y_true,y_prob)\n",
    "    plt.plot(fpr,tpr,label=my_label)\n",
    "    plt.title('ROC curve')\n",
    "    plt.xlabel('fpr')\n",
    "    plt.ylabel('tpr')\n",
    "    \n",
    "    #precision, recall, thresholds = metrics.precision_recall_curve(y_true,y_prob)\n",
    "    #plt.plot(precision,recall,label='precison_and_recall')\n",
    "    plt.legend()\n",
    "    \n",
    "    return re\n",
    "\n",
    "def get_polya_data(polya_name):\n",
    "    X,y = get_data(polya_name)\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=22,)\n",
    "    X_train,X_dev,y_train,y_dev = train_test_split(X_train,y_train,test_size=0.25,random_state=22)\n",
    "    return X_train,X_dev,X_test,y_train,y_dev,y_test\n",
    "\n",
    "X_train,X_dev,X_test,y_train,y_dev,y_test = [],[],[],[],[],[]\n",
    "for index,poly in enumerate(polys):\n",
    "    X_tr,X_de,X_te,y_tr,y_de,y_te = get_polya_data(poly)\n",
    "    if(index == 0):\n",
    "        X_train,X_dev,X_test,y_train,y_dev,y_test = X_tr,X_de,X_te,y_tr,y_de,y_te\n",
    "    else:\n",
    "        X_train = np.concatenate((X_train,X_tr))\n",
    "        X_dev   = np.concatenate((X_dev,X_de))\n",
    "        X_test  = np.concatenate((X_test,X_te))\n",
    "        y_train = np.append(y_train,y_tr)\n",
    "        y_dev   = np.append(y_dev,y_de)\n",
    "        y_test  = np.append(y_test,y_te)\n",
    "        \n",
    "# def my_shuffle(*args):\n",
    "#     for arg in args:\n",
    "#         np.random.seed(22)\n",
    "#         np.random.shuffle(arg)\n",
    "# my_shuffle(X_train,X_dev,X_test,y_train,y_dev,y_test)\n",
    "\n",
    "# def change_y(y):\n",
    "#     re = np.zeros((len(y),2))\n",
    "#     for index,value  in enumerate(y):\n",
    "#         if(value == 1):\n",
    "#             re[index] = [1,0]\n",
    "#         else:\n",
    "#             re[index] = [0,1]\n",
    "#     return re\n",
    "# y_train,y_dev,y_test = change_y(y_train),change_y(y_dev),change_y(y_test)\n",
    "\n",
    "print('X:',X_train.shape,X_dev.shape,X_test.shape)\n",
    "print('Y:',y_train.shape,y_dev.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train,y_train)).batch(32)\n",
    "dev_ds   = tf.data.Dataset.from_tensor_slices((X_dev,y_dev)).batch(32)\n",
    "test_ds  = tf.data.Dataset.from_tensor_slices((X_test,y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D,MaxPool1D,Flatten,Dense\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel,self).__init__()\n",
    "        self.conv1 = Conv1D(128,7,activation='relu',kernel_regularizer=regularizers.l2(0.001))\n",
    "        self.maxp1  = MaxPool1D(pool_size=2,strides=2, padding='valid')\n",
    "        \n",
    "        self.conv2 = Conv1D(128,7,activation='relu',kernel_regularizer=regularizers.l2(0.001))\n",
    "        self.maxp2 = MaxPool1D(pool_size=2,strides=2, padding='valid')\n",
    "        \n",
    "        self.conv3 = Conv1D(128,7,activation='relu',kernel_regularizer=regularizers.l2(0.001))\n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        self.dense1 = Dense(128,activation='relu',kernel_regularizer=regularizers.l2(0.002))\n",
    "        # self.dense2 = Dense(2,activation='softmax')\n",
    "        self.dense2 = Dense(1,activation='sigmoid')\n",
    "        \n",
    "    def call(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxp1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxp2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)\n",
    "    \n",
    "model = MyModel()   \n",
    "\n",
    "# model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy']\n",
    "\n",
    "# model.fit(X_train,y_train)\n",
    "\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "@tf.function\n",
    "def train_step(X,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different \n",
    "        # behavior during training inferent(e.g Dropout)\n",
    "        predictions = model(X,training=True)\n",
    "        loss = loss_object(y[:,0],predictions)\n",
    "    gradients = tape.gradient(loss,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(y[:,0],predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step(X,y):\n",
    "    # trainning = False i sonly needed if there are layers with different\n",
    "    # behavior during training inference(e.g Dropout)\n",
    "    predictions = model(X,training=False)\n",
    "    t_loss = loss_object(y[:,0],predictions)\n",
    "    \n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(y[:,0],predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for X,y in train_ds:\n",
    "        train_step(X,y)\n",
    "    \n",
    "    for X,y in dev_ds:\n",
    "        test_step(X,y)\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy.result()}, '\n",
    "        f'Test Loss: {test_loss.result()}, '\n",
    "        f'Test Accuracy: {test_accuracy.result()}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BinaryCrossentropy' object has no attribute 'reset_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3c97bf30c7e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Reset the metrics at the start of the next epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BinaryCrossentropy' object has no attribute 'reset_states'"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "    for X,y in train_ds:\n",
    "        train_step(X,y)\n",
    "    \n",
    "    for X,y in dev_ds:\n",
    "        test_step(X,y)\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy.result()}, '\n",
    "        f'Test Loss: {test_loss.result()}, '\n",
    "        f'Test Accuracy: {test_accuracy.result()}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.binary_crossentropy\n",
    "loss_object.res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
